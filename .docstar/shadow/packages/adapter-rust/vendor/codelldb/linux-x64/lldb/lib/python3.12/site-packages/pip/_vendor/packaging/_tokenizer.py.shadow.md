# packages/adapter-rust/vendor/codelldb/linux-x64/lldb/lib/python3.12/site-packages/pip/_vendor/packaging/_tokenizer.py
@source-hash: 27abf91fb273bdbf
@generated: 2026-02-09T18:01:34Z

## Python Package Version Specification Tokenizer

**Primary Purpose**: Lexical analyzer for parsing Python package version specifications and dependency declarations, part of the `pip._vendor.packaging` module.

### Core Components

**Token (L11-16)**: Simple dataclass representing a lexical token with name, text content, and source position. Used as the fundamental unit of tokenization output.

**ParserSyntaxError (L18-37)**: Custom exception for tokenization failures. Provides rich error reporting with source highlighting - the `__str__` method (L34-36) generates visual error markers showing exact failure location with tildes and caret.

**DEFAULT_RULES (L39-87)**: Comprehensive token definition dictionary mapping token names to regex patterns. Includes:
- Structural tokens: parentheses, brackets, semicolons, commas
- Operators: comparison operators (===, ==, ~=, etc.), boolean operators (and, or)
- Python environment variables: `python_version`, `os_name`, `sys_platform`, etc.
- Version specifiers: constructed from `Specifier` class patterns (L76-79)
- Identifiers, URLs, whitespace, and end-of-input markers

**Tokenizer (L90-194)**: Main lexical analyzer class implementing context-sensitive parsing.

### Key Methods

**`__init__` (L97-108)**: Initializes tokenizer with source string and rule set. Pre-compiles all regex patterns for performance. Maintains current position and next token state.

**`check` (L115-134)**: Core token matching method. Tests if next token matches given rule without consuming it (unless `peek=True`). Enforces single-token lookahead invariant with assertions.

**`consume` (L110-113)**: Convenience method combining check and read operations.

**`expect` (L136-143)**: Enforces required token presence, raising syntax error with custom message if not found.

**`read` (L145-153)**: Consumes and returns the current next_token, advancing position by token length.

**`raise_syntax_error` (L155-171)**: Error factory creating ParserSyntaxError with optional custom span positioning.

**`enclosing_tokens` (L173-194)**: Context manager for parsing balanced token pairs (parentheses, brackets). Handles optional opening tokens and validates proper closure.

### Architecture Patterns

- **Single-token lookahead**: Enforces `next_token` is consumed before checking new tokens
- **Lazy compilation**: Regex patterns compiled once during initialization
- **Position tracking**: Maintains character-level source position for error reporting
- **Context management**: Provides structured parsing of nested constructs

### Dependencies

- Imports `Specifier` class for version specifier pattern construction
- Uses standard library: `re`, `contextlib`, `dataclasses`, `typing`

### Critical Invariants

- Only one token can be "peeked" at a time before consumption
- Token rules must be pre-defined in the rules dictionary
- Position tracking must remain synchronized with token consumption
- Error spans must be valid source string indices