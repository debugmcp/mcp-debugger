# packages/adapter-rust/vendor/codelldb/linux-arm64/lldb/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py
@source-hash: 2077bd79988a4c5c
@generated: 2026-02-09T18:00:59Z

This module provides the base lexer classes for Pygments syntax highlighting, implementing a hierarchical lexer architecture with metaclass-driven token processing.

## Primary Responsibility
Core lexer framework enabling syntax highlighting through regular expression-based tokenization with stateful processing and extensible filtering capabilities.

## Key Classes

### LexerMeta (L37-46)
Metaclass that automatically converts `analyse_text` methods into static methods via `make_analysator`, ensuring they return float values for lexer confidence scoring.

### Lexer (L49-284)
Base lexer class with standard preprocessing and filtering pipeline:
- **Constructor (L140-168)**: Processes options (`stripnl`, `stripall`, `ensurenl`, `tabsize`, `encoding`) and initializes filter chain
- **_preprocess_lexer_input (L202-249)**: Handles encoding detection/conversion, BOM removal, newline normalization, and text stripping
- **get_tokens (L251-273)**: Main tokenization interface, applies preprocessing and optional filtering
- **get_tokens_unprocessed (L275-284)**: Abstract method for subclasses to implement core tokenization logic

Class attributes define lexer metadata: `name`, `aliases`, `filenames`, `alias_filenames`, `mimetypes`, `priority`, `url`, `_example`.

### DelegatingLexer (L287-318)
Composite lexer that processes text with a language lexer, then applies a root lexer to `Other` tokens. Uses `do_insertions` helper for token stream merging.

### RegexLexerMeta (L496-661)
Complex metaclass for regex-based lexers:
- **Token processing pipeline**: `_process_regex`, `_process_token`, `_process_new_state` methods compile and validate token definitions
- **State inheritance (L601-648)**: `get_tokendefs` merges token definitions from superclasses, handling `inherit` markers
- **Lazy compilation**: Token definitions compiled on first instantiation via `__call__`

### RegexLexer (L664-758)
Stateful regex-based lexer with stack-based state management:
- **get_tokens_unprocessed (L699-758)**: Core tokenization loop with state stack manipulation
- **State transitions**: Supports `#pop`, `#push`, numeric pop counts, and tuple state sequences
- **Error handling**: Falls back to character-by-character Error tokens when no patterns match

### ExtendedRegexLexer (L777-846)
Context-aware version of RegexLexer using `LexerContext` objects for position and state tracking, enabling more sophisticated callback mechanisms.

### LexerContext (L761-774)
Helper object storing lexer state: text position, end boundary, and state stack.

## Key Helper Classes and Functions

### Token Definition Helpers
- **include (L326-330)**: State inclusion marker
- **inherit (L333-340)**: Superclass state inheritance marker  
- **combined (L343-354)**: Multi-state combination utility
- **default (L468-477)**: Default state transition action
- **words (L480-494)**: Optimized word list matching via `regex_opt`

### Callback Generators
- **bygroups (L383-406)**: Multi-action callback for regex capture groups
- **using (L418-465)**: Sub-lexer delegation with state control

### Utility Functions
- **do_insertions (L849-910)**: Merges multiple token streams at specified positions
- **_PseudoMatch (L356-381)**: Mock regex match object for callback processing

## Configuration and Constants
- **line_re (L26)**: Global newline-terminated line pattern
- **_encoding_map (L28-32)**: BOM-to-encoding mapping for automatic detection
- **_default_analyse (L34)**: Fallback analysis function returning 0.0 confidence

## Profiling Support
- **ProfilingRegexLexerMeta/ProfilingRegexLexer (L913-961)**: Development tools for regex performance analysis

## Architectural Patterns
- **Metaclass preprocessing**: Automatic compilation and validation of lexer definitions
- **State machine**: Stack-based state transitions with inheritance and combination
- **Filter pipeline**: Extensible post-processing via filter chain
- **Lazy initialization**: Token definitions compiled on demand for performance