# packages/adapter-rust/vendor/codelldb/linux-arm64/lldb/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py
@source-hash: 0f37870665b52314
@generated: 2026-02-09T17:58:00Z

## Primary Purpose
Implements Pygments lexers for Python and related languages (Python 2.x, 3.x, Cython, dg, NumPy). This file is part of the Pygments syntax highlighting library within pip's vendored dependencies.

## Key Classes and Functions

### PythonLexer (L26-413)
Primary lexer for Python 3.x source code syntax highlighting. Inherits from RegexLexer.
- **Purpose**: Tokenizes Python 3 code including f-strings, async/await, pattern matching
- **Key attributes**: `name`, `aliases`, `filenames`, `mimetypes` (L37-63)
- **Core functionality**: `tokens` dict with regex rules for all Python constructs (L101-407)
- **Text analysis**: `analyse_text()` method for auto-detection (L409-411)

### Helper Functions
- **innerstring_rules(ttype)** (L67-85): Returns regex rules for string interpolation patterns
- **fstring_rules(ttype)** (L87-99): Returns regex rules specific to f-string formatting

### Python2Lexer (L417-637)
Lexer for Python 2.x with differences from Python 3:
- No f-strings, different string prefixes, backtick expressions
- Includes Python 2 specific builtins and magic methods
- **innerstring_rules()** (L433-444): Python 2 string formatting patterns

### Console and Traceback Lexers
- **_PythonConsoleLexerBase** (L638-672): Base for console session highlighting
- **PythonConsoleLexer** (L674-718): Delegating lexer for interactive Python sessions
- **PythonTracebackLexer** (L720-771): Handles Python 3 traceback formatting with chained exceptions
- **Python2TracebackLexer** (L777-822): Python 2 traceback formatting

### CythonLexer (L825-1003)
Extends Python lexer for Cython/Pyrex syntax:
- Additional keywords: `cdef`, `cpdef`, `nogil`, `gil`, etc.
- C type annotations and memory views
- Cython-specific builtins like `bint`, `Py_ssize_t`

### DgLexer (L1005-1102)
Lexer for the dg functional programming language:
- Functional programming constructs
- Distinct operator and keyword patterns
- Custom string and number handling

### NumPyLexer (L1104-1198)
Extends PythonLexer to recognize NumPy-specific functions:
- **EXTRA_KEYWORDS** (L1119-1184): Comprehensive set of NumPy function names
- **get_tokens_unprocessed()** (L1186-1192): Overrides to highlight NumPy functions as pseudo-keywords
- **analyse_text()** (L1194-1198): Auto-detection based on NumPy imports

## Token Structure
All lexers use a comprehensive `tokens` dictionary with states:
- `'root'`: Main parsing state
- `'expr'`: Expression parsing with string literals
- `'keywords'`, `'builtins'`, `'numbers'`: Specific token categories
- String states: `'dqs'`, `'sqs'`, `'tdqs'`, `'tsqs'` for different quote types
- `'import'`, `'fromimport'`: Import statement handling

## Dependencies
- `re`, `keyword` (standard library)
- `pip._vendor.pygments.lexer`: Core lexer classes and utilities
- `pip._vendor.pygments.token`: Token type definitions
- `pip._vendor.pygments.util`: Utility functions like `shebang_matches`
- `pip._vendor.pygments.unistring`: Unicode identifier support

## Architectural Patterns
- **State machine approach**: Uses regex rules with state transitions
- **Inheritance hierarchy**: NumPyLexer extends PythonLexer, specialized behavior through overrides
- **Delegating pattern**: Console lexer delegates to appropriate Python version lexer
- **Rule composition**: Uses `include()`, `combined()`, `bygroups()` for rule reuse

## Critical Invariants
- All lexers maintain proper state transitions with `#pop` operations
- String escape sequences handled consistently across Python versions
- Magic method and builtin recognition uses negative lookbehind `(?<!\.)` to avoid false matches on attributes
- F-string parsing maintains nested expression state tracking