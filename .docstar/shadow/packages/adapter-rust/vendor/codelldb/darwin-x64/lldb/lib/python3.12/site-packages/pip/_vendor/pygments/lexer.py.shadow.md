# packages/adapter-rust/vendor/codelldb/darwin-x64/lldb/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py
@source-hash: 2077bd79988a4c5c
@generated: 2026-02-09T18:00:01Z

This file defines the core lexer framework for the Pygments syntax highlighting library, providing base classes and utilities for creating language lexers.

## Core Architecture

**LexerMeta (L37-46)**: Metaclass that automatically converts `analyse_text` methods into static analyzers that return floats for lexer probability scoring.

**Lexer (L49-284)**: Base lexer class providing fundamental lexing infrastructure:
- Constructor processes common options: stripnl, stripall, ensurenl, tabsize, encoding (L140-168)
- `_preprocess_lexer_input()` handles encoding detection, BOM removal, newline normalization (L202-249)
- `get_tokens()` main interface that preprocesses text and applies filters (L251-273)
- `get_tokens_unprocessed()` abstract method for subclasses to implement (L275-284)
- `add_filter()` for chaining token stream filters (L177-183)

**DelegatingLexer (L287-318)**: Combines two lexers - scans with language lexer first, then processes "Other" tokens with root lexer. Uses `do_insertions()` helper for token stream merging.

## Regex-Based Lexing Framework

**RegexLexerMeta (L496-662)**: Complex metaclass for regex lexer preprocessing:
- `_process_regex()` compiles regex patterns (L502-506)
- `_process_token()` validates token types (L508-512)  
- `_process_new_state()` handles state transitions (#pop, #push, combined states) (L514-547)
- `get_tokendefs()` merges token definitions from class hierarchy with inheritance support (L601-648)
- `process_tokendef()` preprocesses complete token definition dictionaries (L593-599)

**RegexLexer (L664-758)**: State machine-based lexer using regex matching:
- Maintains state stack starting with 'root' state (L675-697)
- `get_tokens_unprocessed()` core tokenization loop with state transitions (L699-758)
- Handles special states: #pop, #push, combined states
- Falls back to Error tokens for unmatched input

**ExtendedRegexLexer (L777-846)**: Context-aware version using LexerContext objects for state management instead of simple stacks.

## State Management Utilities

**LexerContext (L761-774)**: Helper object storing lexer position, text bounds, and state stack.

**include (L326-330)**: Marker class for including rules from other states.

**combined (L343-354)**: Tuple subclass for creating anonymous combined states.

**default (L468-477)**: State action wrapper for default transitions.

**words (L480-494)**: Optimized regex generator for literal word lists using `regex_opt()`.

## Token Processing Helpers

**bygroups (L383-406)**: Callback generator that applies different actions to regex capture groups.

**using (L418-465)**: Callback for delegating token processing to different lexers, supports state specification and lexer options.

**_PseudoMatch (L356-381)**: Mock match object for simulating regex matches from strings.

**do_insertions (L849-911)**: Complex helper for merging token streams from multiple sublexers at specified positions.

## Profiling Support

**ProfilingRegexLexerMeta (L913-932)** and **ProfilingRegexLexer (L935-961)**: Drop-in replacement that collects timing data for regex matching performance analysis.

## Global Constants

- `line_re` (L26): Regex for line matching
- `_encoding_map` (L28-32): BOM to encoding mappings for auto-detection
- `_default_analyse` (L34): Default analyzer returning 0.0 probability